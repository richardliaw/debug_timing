{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Workers - 2k - 1 thread\n",
    "```\n",
    "Total time: 37.6897 s\n",
    "File: driver.py\n",
    "Function: train at line 61\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "    61                                           def train(num_workers, env_name=\"PongDeterministic-v3\"):\n",
    "    62         1        33704  33704.0      0.1    env = create_env(env_name)\n",
    "    63         1      1460827 1460827.0      3.9    policy = LSTMPolicy(env.observation_space.shape, env.action_space.n, 0)\n",
    "    64         1          425    425.0      0.0    policy.setup_async()\n",
    "    65         1        41168  41168.0      0.1    agents = [Runner.remote(env_name, i) for i in range(num_workers)]\n",
    "    66         1         4896   4896.0      0.0    parameters = policy.get_weights()\n",
    "    67         1            3      3.0      0.0    gradient_list = [agent.compute_gradient.remote(parameters)\n",
    "    68         1        22949  22949.0      0.1                     for agent in agents]\n",
    "    69         1            6      6.0      0.0    steps = 0\n",
    "    70         1            4      4.0      0.0    obs = 0\n",
    "    71         1            5      5.0      0.0    timing = []\n",
    "    72      2001        11726      5.9      0.0    for i in range(2000):\n",
    "    73      2000     26694132  13347.1     70.8      done_id, gradient_list = ray.wait(gradient_list)\n",
    "    74      2000      1613321    806.7      4.3      gradient, info = ray.get(done_id)[0]\n",
    "    75      2000      1037952    519.0      2.8      policy.async_model_update(gradient)\n",
    "    76      2000       178842     89.4      0.5      parameters = policy.get_weights(cached=True)\n",
    "    77      2000         7586      3.8      0.0      obs += info[\"size\"]\n",
    "    78      2000         5335      2.7      0.0      gradient_list.extend(\n",
    "    79      2000      6576790   3288.4     17.4          [agents[info[\"id\"]].compute_gradient.remote(parameters)])\n",
    "    80         1            3      3.0      0.0    return policy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 Workers - 2k - 1 thread\n",
    "\n",
    "```\n",
    "Total time: 36.6105 s\n",
    "File: driver.py\n",
    "Function: train at line 61\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "    61                                           def train(num_workers, env_name=\"PongDeterministic-v3\"):\n",
    "    62         1        33337  33337.0      0.1    env = create_env(env_name)\n",
    "    63         1      1420746 1420746.0      3.9    policy = LSTMPolicy(env.observation_space.shape, env.action_space.n, 0)\n",
    "    64         1          542    542.0      0.0    policy.setup_async()\n",
    "    65         1        59204  59204.0      0.2    agents = [Runner.remote(env_name, i) for i in range(num_workers)]\n",
    "    66         1         6355   6355.0      0.0    parameters = policy.get_weights()\n",
    "    67         1           12     12.0      0.0    gradient_list = [agent.compute_gradient.remote(parameters)\n",
    "    68         1       252549 252549.0      0.7                     for agent in agents]\n",
    "    69         1            3      3.0      0.0    steps = 0\n",
    "    70         1            3      3.0      0.0    obs = 0\n",
    "    71         1            2      2.0      0.0    timing = []\n",
    "    72      2001        10680      5.3      0.0    for i in range(2000):\n",
    "    73      2000      4961003   2480.5     13.6      done_id, gradient_list = ray.wait(gradient_list)\n",
    "    74      2000      1827644    913.8      5.0      gradient, info = ray.get(done_id)[0]\n",
    "    75      2000     20369259  10184.6     55.6      policy.async_model_update(gradient)\n",
    "    76      2000       106331     53.2      0.3      parameters = policy.get_weights(cached=True)\n",
    "    77      2000         9491      4.7      0.0      obs += info[\"size\"]\n",
    "    78      2000         5953      3.0      0.0      gradient_list.extend(\n",
    "    79      2000      7547378   3773.7     20.6          [agents[info[\"id\"]].compute_gradient.remote(parameters)])\n",
    "    80         1            3      3.0      0.0    return policy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 Workers - 2k - 2 threads\n",
    "```\n",
    "Timer unit: 1e-06 s\n",
    "\n",
    "Total time: 33.2558 s\n",
    "File: driver.py\n",
    "Function: train at line 61\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "    61                                           def train(num_workers, env_name=\"PongDeterministic-v3\"):\n",
    "    62         1        33657  33657.0      0.1    env = create_env(env_name)\n",
    "    63         1      1451611 1451611.0      4.4    policy = LSTMPolicy(env.observation_space.shape, env.action_space.n, 0)\n",
    "    64         1          763    763.0      0.0    policy.setup_async()\n",
    "    65         1        75194  75194.0      0.2    agents = [Runner.remote(env_name, i) for i in range(num_workers)]\n",
    "    66         1         9292   9292.0      0.0    parameters = policy.get_weights()\n",
    "    67         1            9      9.0      0.0    gradient_list = [agent.compute_gradient.remote(parameters)\n",
    "    68         1       188078 188078.0      0.6                     for agent in agents]\n",
    "    69         1           10     10.0      0.0    steps = 0\n",
    "    70         1            3      3.0      0.0    obs = 0\n",
    "    71         1            4      4.0      0.0    timing = []\n",
    "    72      2001        28449     14.2      0.1    for i in range(2000):\n",
    "    73      2000      5814879   2907.4     17.5      done_id, gradient_list = ray.wait(gradient_list)\n",
    "    74      2000      2890294   1445.1      8.7      gradient, info = ray.get(done_id)[0]\n",
    "    75      2000     12932430   6466.2     38.9      policy.async_model_update(gradient)\n",
    "    76      2000       578172    289.1      1.7      parameters = policy.get_weights(cached=True)\n",
    "    77      2000        10619      5.3      0.0      obs += info[\"size\"]\n",
    "    78      2000         5836      2.9      0.0      gradient_list.extend(\n",
    "    79      2000      9236533   4618.3     27.8          [agents[info[\"id\"]].compute_gradient.remote(parameters)])\n",
    "    80         1            3      3.0      0.0    return policy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 Workers - 2k - 3 threads\n",
    "```\n",
    "Total time: 36.4138 s\n",
    "File: driver.py\n",
    "Function: train at line 61\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "    61                                           def train(num_workers, env_name=\"PongDeterministic-v3\"):\n",
    "    62         1        33781  33781.0      0.1    env = create_env(env_name)\n",
    "    63         1      1452658 1452658.0      4.0    policy = LSTMPolicy(env.observation_space.shape, env.action_space.n, 0)\n",
    "    64         1          832    832.0      0.0    policy.setup_async()\n",
    "    65         1        65122  65122.0      0.2    agents = [Runner.remote(env_name, i) for i in range(num_workers)]\n",
    "    66         1         6565   6565.0      0.0    parameters = policy.get_weights()\n",
    "    67         1            7      7.0      0.0    gradient_list = [agent.compute_gradient.remote(parameters)\n",
    "    68         1       245772 245772.0      0.7                     for agent in agents]\n",
    "    69         1            5      5.0      0.0    steps = 0\n",
    "    70         1            2      2.0      0.0    obs = 0\n",
    "    71         1            5      5.0      0.0    timing = []\n",
    "    72      2001        30704     15.3      0.1    for i in range(2000):\n",
    "    73      2000      5724680   2862.3     15.7      done_id, gradient_list = ray.wait(gradient_list)\n",
    "    74      2000      2806039   1403.0      7.7      gradient, info = ray.get(done_id)[0]\n",
    "    75      2000     13767336   6883.7     37.8      policy.async_model_update(gradient)\n",
    "    76      2000       566464    283.2      1.6      parameters = policy.get_weights(cached=True)\n",
    "    77      2000        10111      5.1      0.0      obs += info[\"size\"]\n",
    "    78      2000         6353      3.2      0.0      gradient_list.extend(\n",
    "    79      2000     11697359   5848.7     32.1          [agents[info[\"id\"]].compute_gradient.remote(parameters)])\n",
    "    80         1            3      3.0      0.0    return policy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
